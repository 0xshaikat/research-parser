{
  "query": "large language model multi-agent security risks alignment adversarial attacks safety vulnerabilities coordination autonomous agents collaborative AI",
  "threshold": 0.6,
  "total_papers_analyzed": 20,
  "ranked_papers": [
    {
      "title": "A Prototype Design of LLM-Based Autonomous Web Crowdsensing",
      "authors": "Zhu, Zhengqiu and Ji, Yatai and Qiu, Sihang and Zhao, Yong and Xu, Kai and Ju, Rusheng and Chen, Bin",
      "year": "2024",
      "doi": "10.1007/978-3-031-62362-2_34",
      "filepath": "papers/A Prototype Design of LLM-Based Autonomous Web Crowdsensing_2024.pdf",
      "relevance_score": 0.6468943506479263,
      "relevant_paragraphs": [
        {
          "content": "testing, and red teaming.7 We refer to these adversarial testing processes informally as \u201cred teaming\u201d\nin line with the definition given in [27], namely\u201ca structured effort to find flaws and vulnerabilities\nin a plan, organization, or technical system, often performed by dedicated \u2019red teams\u2019 that seek to\nadopt an attacker\u2019s mindset and methods.\u201d We conducted internal adversarial testing GPT-4-launch\non March 10, 2023. We also tested multiple similar versions of GPT-4 in the lead-up to this\ndate, so analysis here is informed by that exploration as well. Red teaming has been applied to\nlanguage models in various ways: to reduce harmful outputs;[28] and to leverage external expertise\nfor domain-specific adversarial testing.[16] Some have explored red teaming language models using\nlanguage models.[29]\nRed teaming in general, and the type of red teaming we call \u2019expert red teaming,\u20198 is just one of\nthe mechanisms[27] we use to inform our work identifying, measuring, and testing AI systems. Our\napproach is to red team iteratively, starting with an initial hypothesis of which areas may be the\nhighest risk, testing these areas, and adjusting as we go. It is also iterative in the sense that we\nuse multiple rounds of red teaming as we incorporate new layers of mitigation and control, conduct\ntesting and refining, and repeat this process.\nWe reached out to researchers and industry professionals - primarily with expertise in fairness,\nalignment research, industry trust and safety, dis/misinformation, chemistry, biorisk, cybersecurity,\nnuclear risks, economics, human-computer interaction, law, education, and healthcare - to help\nus gain a more robust understanding of the GPT-4 model and potential deployment risks. We\nselected these areas based on a number of factors including but not limited to: prior observed risks in\nlanguage models and AI systems;[6, 30] and domains where we have observed increased user interest\nin the application of language models. Participants in this red team process were chosen based on\nprior research or experience in these risk areas, and therefore reflect a bias towards groups with\nspecific educational and professional backgrounds (e.g., people with significant higher education or\nindustry experience). Participants also typically have ties to English-speaking, Western countries\n(such as the US, Canada, and the UK). Our selection of red teamers introduces some biases, and\nlikely influenced both how red teamers interpreted particular risks as well as how they probed\npolitics, values, and the default behavior of the model. It is also likely that our approach to sourcing\nresearchers privileges the kinds of risks that are top of mind in academic communities and at AI\nfirms.\nThese experts had access to early versions of GPT-4 (including GPT-4-early) and to the model\nwith in-development mitigations (precursors to GPT-4-launch). They identified initial risks that\nmotivated safety research and further iterative testing in key areas. We reduced risk in many of\nthe identified areas with a combination of technical mitigations, and policy and enforcement levers;\nhowever, many risks still remain. We expect to continue to learn more about these and other\ncategories of risk over time. While this early qualitative red teaming exercise is very useful for\ngaining insights into complex, novel models like GPT-4, it is not a comprehensive evaluation of all\npossible risks.\nWe note further context, examples, and findings for some of the domains evaluated in the\nremainder in the subcategories listed in this section.\n7Note that, in addition to red teaming focused on probing our organization\u2019s capabilities and resilience to attacks,\nwe also make ample use of stress testing and boundary testing methods which focus on surfacing edge cases and other\npotentialfailuremodeswithpotentialtocauseharm. Inordertoreduceconfusionassociatedwiththeterm\u2019redteam\u2019,\nhelp those reading about our methods to better contextualize and understand them, and especially to avoid false\nassurances, we are working to adopt clearer terminology, as advised in [26], however, for simplicity and in order to use\nlanguage consistent with that we used with our collaborators, we use the term \u201cred team\u201d in this document.\n8We use the term \u2019expert\u2019 to refer to expertise informed by a range of domain knowledge and lived experiences.\n45",
          "similarity": 0.668777346611023
        },
        {
          "content": "based on a number of factors, including prior observed risks in language models and AI systems,\nand domains where we have observed increased user interest in the application of language models.\nWorkingwiththeseexpertsenabledustotestmodelbehaviorinhigh-riskareasthatrequireexpertise\nto evaluate, as well as nascent risks that are poorly understood.\nThrough this analysis, we find that GPT-4 has the potential to be used to attempt to identify\nprivate individuals when augmented with outside data. We also find that, although GPT-4\u2019s\ncybersecurity capabilities are not vastly superior to previous generations of LLMs, it does continue\nthe trend of potentially lowering the cost of certain steps of a successful cyberattack, such as through\nsocial engineering or by enhancing existing security tools. Without safety mitigations, GPT-4 is\nalso able to give more detailed guidance on how to conduct harmful or illegal activities. Finally, we\nfacilitated a preliminary model evaluation by the Alignment Research Center (ARC) of GPT-4\u2019s\nability to carry out actions to autonomously replicate5 and gather resources\u2014a risk that, while\nspeculative, may become possible with sufficiently advanced AI systems\u2014with the conclusion that\nthe current model is probably not yet capable of autonomously doing so.\nFurther research is needed to fully characterize these risks. In particular, we would like to see\nwork on more robust evaluations for the risk areas identified and more concrete measurements of the\nprevalence of such behaviors across different language models, and to guide the development of these\nmodels in safer directions. We are working on these types of evaluations, often in collaboration with\nother research groups, with a focus on assessing risky emergent behaviors.\nIn addition to work on measurement, we aimed to mitigate the identified issues at various steps\nof the development and deployment process. We reduced the prevalence of certain kinds of content\nthat violate our usage policies (such as inappropriate erotic content) in our pre-training dataset, and\nfine-tuned the model to refuse certain instructions such as direct requests for illicit advice. We also\nreduced the tendency of the models to hallucinate and, by leveraging data from prior model usage,\nreduced the surface area of adversarial prompting or exploits (including attacks sometimes referred\nto as \u201cjailbreaks\u201d) that the model succumbs to. Additionally, we trained a range of classifiers on\nnew risk vectors and have incorporated these into our monitoring workflow, enabling us to better\nenforce our API usage policies. The effectiveness of these mitigations varies, but overall we were able\nto significantly reduce the ease of producing various kinds of potentially harmful content, thereby\nmaking GPT-4-launch significantly safer than GPT-4-early along these dimensions.\nThis system card is not comprehensive, and we expect to learn more over time about the\nissues discussed below. Consistent with OpenAI\u2019s deployment strategy,[21] we applied lessons from\nearlier deployments and expect to apply lessons learned from this deployment both to make course\ncorrections and lay a foundation for future deployments.\nNote that the examples included throughout this system card are not zero-shot and are cherry\npicked from our evaluation efforts to illustrate specific types of safety concerns or harms. We included\nexamples to provide readers with context about the nature of the observed risks. One example is\nnot enough to show the breadth of ways these issues may manifest.\nIn Section 1, we outline some of the observed safety challenges in the development of GPT-4. In\nSection 2, we discuss our process for deployment preparation and some of the model mitigations and\nsystem safety measures. In Section 3, we conclude by discussing some remaining limitations and\nrecommendations in light of the observed risks we have learned through our iterative deployment\nstrategy.\n5Autonomously replicate is a reference to self-replication, a concept that dates back at least as far as the 1988, to\nthe self-replicating computer worms, \u201cMorris worm\u201d, written by Robert Morris.[20]\n43",
          "similarity": 0.6579796075820923
        },
        {
          "content": "safe usage.\n\u2022 Build evaluations, mitigations, and approach deployment with real-world usage\nin mind: Context of use such as who the users are, what the specific use case is, where the\nmodel is being deployed, etc., is critical to mitigating actual harms associated with language\nmodels and ensuring their deployment is as beneficial as possible. It\u2019s particularly important to\naccount for real-world vulnerabilities, humans roles in the deployment context, and adversarial\nattempts. We especially encourage the development of high quality evaluations and testing of\nmodel mitigations on datasets in multiple languages.\n\u2022 Ensure that safety assessments cover emergent risks: As models get more capable, we\nshouldbepreparedforemergentcapabilitiesandcomplexinteractionstoposenovelsafetyissues.\nIt\u2019s important to develop evaluation methods that can be targeted at advanced capabilities that\ncould be particularly dangerous if they emerged in future models, while also being open-ended\nenough to detect unforeseen risks.\n\u2022 Be cognizant of, and plan for, capability jumps \u201cin the wild\u201d: Methods like fine-tuning\nand chain-of-thought prompting could lead to capability jumps in the same base model. This\nshould be accounted for explicitly in internal safety testing procedures and evaluations. And\na precautionary principle should be applied: above a safety critical threshold, assurance of\nsufficient safety is required.\nThe increase in capabilities and adoption of these models have made the challenges and conse-\nquences of those challenges outlined in this card imminent. As a result, we especially encourage\nmore research into:\n\u2022 Economic impacts of AI and increased automation, and the structures needed to make the\ntransition for society smoother\n\u2022 Structures that allow broader public participation into decisions regarding what is considered\nthe \u201coptimal\u201d behavior for these models\n\u2022 Evaluations for risky emergent behaviors, such as situational awareness, persuasion, and\nlong-horizon planning\n\u2022 Interpretability, explainability, and calibration, to address the current nature of \u201cblack-box\u201d\nAI models. We also encourage research into effective means of promoting AI literacy to aid\nappropriate scrutiny to model outputs.\nAs we see above, both improved language model capabilities and limitations can pose significant\nchallenges to the responsible and safe societal adoption of these models. To ensure that we are all\nwell-prepared for the pace of progress, we need more research emphasis on areas such as AI literacy,\neconomic and social resilience, and anticipatory governance.[11] It is very important that OpenAI,\nother labs, and academia further develop effective evaluation tools and technical improvements in\nmodel safety. Progress has been made in the last few years, and more investment in safety will likely\nproduce more gains.\nWe encourage readers interested in this topic to read our work on language model impacts in\nareas such as disinformation, misuse, education, and economy and labor market.\n69",
          "similarity": 0.6542118191719055
        },
        {
          "content": "GPT-4 System Card\nOpenAI\nAbstract\nLarge language models (LLMs) are being deployed in many domains of our lives ranging\nfrom browsing, to voice assistants, to coding assistance tools, and have potential for vast societal\nimpacts.[1, 2, 3, 4, 5, 6, 7] This system card analyzes GPT-4, the latest LLM in the GPT family\nof models.[8, 9, 10] First, we highlight safety challenges presented by the model\u2019s limitations\n(e.g., producing convincing text that is subtly false) and capabilities (e.g., increased adeptness\nat providing illicit advice, performance in dual-use capabilities, and risky emergent behaviors).\nSecond, we give a high-level overview of the safety processes OpenAI adopted to prepare GPT-4\nfor deployment. This spans our work across measurements, model-level changes, product- and\nsystem-level interventions (such as monitoring and policies), and external expert engagement.\nFinally, we demonstrate that while our mitigations and processes alter GPT-4\u2019s behavior and\nprevent certain kinds of misuses, they are limited and remain brittle in some cases. This points\nto the need for anticipatory planning and governance.[11]\nContent Warning: This document contains content that some may find disturbing or offensive,\nincluding content that is sexual, hateful, or violent in nature.\n41",
          "similarity": 0.6066086292266846
        }
      ],
      "paragraph_count": 97
    },
    {
      "title": "Opportunities and Challenges in the Cultivation of Software Development Professionals in the Context of Large Language Models",
      "authors": "Chen, Ping and Alias, Syazwina Binti",
      "year": "2024",
      "doi": "10.1145/3700297.3700342",
      "filepath": "papers/Opportunities and Challenges in the Cultivation of Software Development Professionals in the Context of Large Language Models_2024.pdf",
      "relevance_score": 0.6372509598731995,
      "relevant_paragraphs": [
        {
          "content": "81\nasAgentVerse[969]andAutoGen[970],canalsobeutilized RobustnessandTrustworthiness.ThedeploymentofLLM-\nfor developing multi-agent collaborative systems. In the based agent systems necessitates robustness and trustwor-\ncompetition-based mode, debate serves as one of the pop- thiness [973]. The system should be resilient against adver-\nular communication protocols to foster divergent thinking sarial inputs from various modalities such as text, image,\nand elicit valuable external feedback among agents. Such a oraudio.Incorporatingexistingtechniqueslikeadversarial\nwayisbeneficialfordomainsthatdemandprecisedecision- training, data augmentation, and sample detection to in-\nmaking and accurate responses, such as mathematical rea- creasesensitivitytoaggressiveinformationintheinputcan\nsoning[971]andevaluation[734]. fortify the system\u2019s security. Concurrently, it is challenging\nto ensure the credibility of LLM-based agents given the se-\n9.2.3 Discussion vere hallucination issues inherently rooted in LLMs. While\nDespitethehugesuccess,therestillremainseveraltechnical existingmethodssuchasconstraineddecodingduringinfer-\nchallenges that limit the development and application of enceandexternalknowledgeintegrationcanmitigatethese\nLLM-based agents. In this part, we discuss the remaining issues to some extent [974], further exploration of efficient\nchallenges from the perspective of computational burden, and effective alignment methods is necessary to develop\nhuman alignment, complex capability extension, and ro- reliableagentsystems.\nbustness.\n9.3 AnalysisandOptimizationforModelTraining\nComputational Costs. With the ever-increasing capabilities\nof LLMs [821], their performance on agent applications In Section 4.3, we have introduced basic techniques for\ndemonstrate promising performance. However, it also in- training LLMs. As the scale of model parameters and data\ntroduces significant issues in terms of efficiency due to continuestoexpand,efficientlytraininglargermodelswith\nthe high computational demands and intricate interaction limited computational resources has become a critical tech-\nmechanismsinvolved.Furthermore,inmulti-agentsystems nicalchallengeinthedevelopmentofLLMs.Thischallenge\nwithnumerousLLMinstances,asthenumberofagentsin- primarily encompasses two technical issues: firstly, how\ncreases,thisissuewouldbemoresevere,sincethecommu- to optimize memory usage when loading and processing\nnication network within multi-agent systems also becomes modelsacrossGPUclusters,andsecondly,howtomaintain\nincreasinglycomplex.Therefore,moreeffectiveandefficient or improve training efficiency as models scale. Next, we\ncommunication protocols and architectures are essential will conduct quantitative analyses and introduce advanced\nto support the heightened coordination demands among training techniques addressing the two aforementioned is-\nagents. sues.\nAlignment with Human Sociality. LLM-based agents can 9.3.1 EstimationofTrainingMemoryConsumption\nbeconceptualizedasindividualentities,withtheemergence\nIn this part, we will first estimate the GPU memory con-\nof sociability resulting from the interaction among these\nsumptionfortrainingLLMs.\nagents.Autonomousagentsoftenassumespecificrolessuch\nas coders or researchers, making role-playing a vital capa- Model States Cost.Modelstatesoftenoccupythemajority\nbilityforagentstosolvedownstreamtasks[972].However, of memory during training, typically consisting of model\nLLMs, typically trained on web corpora, face difficulties in parameters, gradients, and optimizer states. As introduced\naccurately mimicking roles that are infrequently discussed in Section 4.3.2, mixed precision training has been widely\nonline or are emergent. They also lack self-awareness in utilizedinLLMtraining.ForamodelcontainingP param-\nconversationalscenariosduetoinadequatemodelingofhu- eters, both the model parameters and their gradients are\nmancognitivepsychology.Thus,itisimperativetodevelop typically stored as 16-bit floating-point numbers, requiring\nimprovedagenttechnique,includingbothtrainingmethods atotalstorageof4P bytes(2P fortheparametersand2P for\nand architectures, to better align LLMs withhuman prefer- the gradients). When using optimizers such as Adam [318]\nencesandenhancetheirrole-playingabilities. or AdamW [975], an additional set of 32-bit floating-point\nnumbersareneededtostoretheoptimizerstates,including\nCapability Extension. LLM-based agents, similar to hu-\nthe copy of model parameters, gradient momenta, and\nmans, require advanced capabilities (e.g., tool learning) to\ngradient variances, which leads to a total storage of 12P\nfulfill complex functions or tasks, which might be beyond\nbytes(4P eachforeachofthesecomponents).Consequently,\ntheir capacity scope. To address this issue, tool use has\nthe total memory required for storing the model states\nbecomeawidely-usedapproachtoenhancingLLMs\u2019capac-\nduringtrainingis16P bytes.Forinstance,trainingLLaMA-\nities in various complex tasks. For example, when answer- 7B(P \u22486.7\u00d7109)requiresaround100GBmemorytostore\ning informative user questions, they use search engines to\nthemodelstatesalone.\nretrieveinformationfromtheinternet.However,thequality\nand quantity of existing available tools impose limitations Activations Cost. Activations are the intermediate states\nontheiraccessibilityandcomprehensiveness.Anditwould that require to be stored in the forward pass for gradient\nbecome more difficult for LLM-based agents to use such computation during backpropagation. For example, for a\nlimited tools when interacting with dynamic and changing binary operation Y = WX, calculating the gradient \u2202Y\n\u2202W\nenvironments. In addition, as the scale of tools expands, necessitates the input X, which should be preserved dur-\nthe compatibility and extensibility between the agents and ing the forward pass. In Table 18, we list the estimation\ntools must be further improved to facilitate complex task of the activation memory consumption for different com-\nresolution. ponents within the Transformer model. Take LLaMA-7B",
          "similarity": 0.6372509598731995
        }
      ],
      "paragraph_count": 144
    },
    {
      "title": "A survey on large language model based autonomous agents",
      "authors": "Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong",
      "year": "2024",
      "doi": "10.1007/s11704-024-40231-1",
      "filepath": "papers/A survey on large language model based autonomous agents_2024.pdf",
      "relevance_score": 0.611679881811142,
      "relevant_paragraphs": [
        {
          "content": "Lei WANG et al. A survey on large language model based autonomous agents 19\nTable 3 For subjective evaluation, we use \u2460 and \u2461 to represent human annotation and the Turing test, respectively. For objective evaluation, we use \u2460, \u2461,\n\u2462, and \u2463 to represent environment simulation, social evaluation, multi-task evaluation, and software testing, respectively. \u201c\u2713\u201d indicates that the evaluations\nare based on benchmarks\nModel Subjective Objective Benchmark Time\nWebShop [80] \u2212 \u2460 \u2462 \u2713 07/2022\nSocial Simulacra [98] \u2460 \u2461 \u2212 08/2022\nTE [102] \u2212 \u2461 \u2212 08/2022\nLIBRO [168] \u2212 \u2463 \u2212 09/2022\nReAct [59] \u2212 \u2460 \u2713 10/2022\nOut of One, Many [29] \u2461 \u2461 \u2462 \u2212 02/2023\nDEPS [33] \u2212 \u2460 \u2713 02/2023\nJalil et al. [169] \u2212 \u2463 \u2212 02/2023\nReflexion [12] \u2212 \u2460 \u2462 \u2212 03/2023\nIGLU [122] \u2212 \u2460 \u2713 04/2023\nGenerative Agents [20] \u2460 \u2461 \u2212 \u2212 04/2023\nToolBench [149] \u2212 \u2462 \u2713 04/2023\nGITM [16] \u2212 \u2460 \u2713 05/2023\nTwo-Failures [162] \u2212 \u2462 \u2212 05/2023\nVoyager [38] \u2212 \u2460 \u2713 05/2023\nSocKET [165] \u2212 \u2461 \u2462 \u2713 05/2023\nMobileEnv [163] \u2212 \u2460 \u2462 \u2713 05/2023\nClembench [173] \u2212 \u2460 \u2462 \u2713 05/2023\nDialop [175] \u2212 \u2461 \u2713 06/2023\nFeldt et al. [170] \u2212 \u2463 \u2212 06/2023\nCO-LLM [22] \u2460 \u2460 \u2212 07/2023\nTachikuma [164] \u2460 \u2460 \u2713 07/2023\nWebArena [171] \u2212 \u2460 \u2713 07/2023\nRocoBench [89] \u2212 \u2460 \u2461 \u2462 \u2212 07/2023\nAgentSims [34] \u2212 \u2461 \u2212 08/2023\nAgentBench [167] \u2212 \u2462 \u2713 08/2023\nBOLAA [166] \u2212 \u2460 \u2462 \u2463 \u2713 08/2023\nGentopia [172] \u2212 \u2462 \u2713 08/2023\nEmotionBench [160] \u2460 \u2212 \u2713 08/2023\nPTB [128] \u2212 \u2463 \u2212 08/2023\n6 Challenges firstly collect real-human data for uncommon roles or\nWhile previous work on LLM-based autonomous agent has psychology characters, and then leverage it to fine-tune LLMs.\nobtained many remarkable successes, this field is still at its However, how to ensure that fine-tuned model still perform\ninitial stage, and there are several significant challenges that well for the common roles may pose further challenges.\nneed to be addressed in its development. In the following, we Beyond fine-tuning, one can also design tailored agent\npresent many representative challenges. prompts/architectures to enhance the capability of LLM on\nrole-playing. However, finding the optimal prompts/\n6.1 Role-playing capability\narchitectures is not easy, since their designing spaces are too\nDifferent from traditional LLMs, autonomous agent usually\nlarge.\nhas to play as specific roles (e.g., program coder, researcher,\nand chemist) for accomplishing different tasks. Thus, the 6.2 Generalized human alignment\ncapability of the agent for role-playing is very important. Human alignment has been discussed a lot for traditional\nAlthough LLMs can effectively simulate many common roles LLMs. In the field of LLM-based autonomous agent,\nsuch as movie reviewers, there are still various roles and especially when the agents are leveraged for simulation, we\naspects that they struggle to capture accurately. To begin with, believe this concept should be discussed more in depth. In\nLLMs are usually trained based on web-corpus, thus for the order to better serve human-beings, traditional LLMs are\nroles which are seldom discussed on the Web or the newly usually fine-tuned to be aligned with correct human values, for\nemerging roles, LLMs may not simulate them well. In example, the agent should not plan to make a bomb for\naddition, previous research [30] has shown that existing LLMs avenging society. However, when the agents are leveraged for\nmay not well model the human cognitive psychology real-world simulation, an ideal simulator should be able to\ncharacters, leading to the lack of self-awareness in honestly depict diverse human traits, including the ones with\nconversation scenarios. Potential solution to these problems incorrect values. Actually, simulating the human negative\nmay include fine-tuning LLMs or carefully designing the aspects can be even more important, since an important goal\nagent prompts/architectures [183]. For example, one can of simulation is to discover and solve problems, and without",
          "similarity": 0.6272901296615601
        },
        {
          "content": "Lei WANG et al. A survey on large language model based autonomous agents 23\ntasks by connecting foundation models with millions of APIs. 2023, preprint arXiv: 2308.12033\narXiv preprint arXiv: 2303.16434 91. Du Y, Li S, Torralba A, Tenenbaum J B, Mordatch I. Improving\n72. Karpas E, Abend O, Belinkov Y, Lenz B, Lieber O, Ratner N, Shoham factuality and reasoning in language models through multiagent\nY, Bata H, Levine Y, Leyton-Brown K, Muhlgay D, Rozen N, debate. 2023, arXiv preprint arXiv: 2305.14325\nSchwartz E, Shachaf G, Shalev-Shwartz S, Shashua A, Tenenholtz M. 92. Zhang C, Yang Z, Liu J, Han Y, Chen X, Huang Z, Fu B, Yu G.\nMRKL systems: a modular, neuro-symbolic architecture that combines AppAgent: multimodal agents as smartphone users. 2023, arXiv\nlarge language models, external knowledge sources and discrete preprint arXiv: 2312.13771\nreasoning. 2022, arXiv preprint arXiv: 2205.00445 93. Madaan A, Tandon N, Clark P, Yang Y. Memory-assisted prompt\n73. Ge Y, Hua W, Mei K, Tan J, Xu S, Li Z, Zhang Y. OpenAGI: When editing to improve GPT-3 after deployment. In: Proceedings of 2022\nLLM meets domain experts. In: Proceedings of the 37th Conference on Conference on Empirical Methods in Natural Language Processing.\nNeural Information Processing Systems, 2023, 36 2022, 2833\u20132861\n74. Sur\u00eds D, Menon S, Vondrick C. ViperGPT: visual inference via python 94. Colas C, Teodorescu L, Oudeyer P Y, Yuan X, C\u00f4t\u00e9 M A.\nexecution for reasoning. 2023, arXiv preprint arXiv: 2303.08128 Augmenting autotelic agents with large language models. In:\n75. Bran A M, Cox S, Schilter O, Baldassari C, White A D, Schwaller P. Proceedings of the 2nd Conference on Lifelong Learning Agents.\nChemCrow: augmenting large-language models with chemistry tools. 2023, 205\u2013226\n2023, arXiv preprint arXiv: 2304.05376 95. Nascimento N, Alencar P, Cowan D. Self-adaptive large language\n76. Yang Z, Li L, Wang J, Lin K, Azarnasab E, Ahmed F, Liu Z, Liu C, model (LLM)-based multiagent systems. In: Proceedings of 2023 IEEE\nZeng M, Wang L. MM-REACT: Prompting chatGPT for multimodal International Conference on Autonomic Computing and Self-\nreasoning and action. 2023, arXiv preprint arXiv: 2303.11381 Organizing Systems Companion. 2023, 104\u2212109\n77. Gao C, Lan X, Lu Z, Mao J, Piao J, Wang H, Jin D, Li Y. S3: social- 96. Saha S, Hase P, Bansal M. Can language models teach weaker agents?\nnetwork simulation system with large language model-empowered Teacher explanations improve students via personalization. 2023,\nagents. 2023, arXiv preprint arXiv: 2307.14984 arXiv preprint arXiv: 2306.09299\n78. Ichter B, Brohan A, Chebotar Y, Finn C, Hausman K, et al. Do as I 97. Zhuge M, Liu H, Faccio F, Ashley D R, Csord\u00e1s R, Gopalakrishnan A,\ncan, not as I say: grounding language in robotic affordances. In: Hamdi A, Hammoud H A A K, Herrmann V, Irie K, Kirsch L, Li B, Li\nProceedings of the 6th Conference on Robot Learning. 2023, 287\u2212318 G, Liu S, Mai J, Pi\u0119kos P, Ramesh A, Schlag I, Shi W, Stani\u0107 A, Wang\n79. Liu H, Sferrazza C, Abbeel P. Chain of hindsight aligns language W, Wang Y, Xu M, Fan D P, Ghanem B, Schmidhuber J. Mindstorms\nmodels with feedback. arXiv preprint arXiv: 2302.02676 in natural language-based societies of mind. 2023, arXiv preprint\n80. Yao S, Chen H, Yang J, Narasimhan K. WebShop: towards scalable arXiv: 2305.17066\nreal-world Web interaction with grounded language agents. In: 98. Park J S, Popowski L, Cai C, Morris M R, Liang P, Bernstein M S.\nProceedings of the 36th Conference on Neural Information Processing Social simulacra: creating populated prototypes for social computing\nSystems. 2022, 20744\u221220757 systems. In: Proceedings of the 35th Annual ACM Symposium on\n81. Dan Y, Lei Z, Gu Y, Li Y, Yin J, Lin J, Ye L, Tie Z, Zhou Y, Wang Y, User Interface Software and Technology. 2022, 74\nZhou A, Zhou Z, Chen Q, Zhou J, He L, Qiu X. EduChat: a large-scale 99. Li G, Hammoud H A A K, Itani H, Khizbullin D, Ghanem B.\nlanguage model-based chatbot system for intelligent education. 2023, CAMEL: communicative agents for \"mind\" exploration of large\narXiv preprint arXiv: 2308.02773 language model society. 2023, arXiv preprint arXiv: 2303.17760\n82. Lin B Y, Fu Y, Yang K, Brahman F, Huang S, Bhagavatula C, 100. AutoGPT. See Github.com/Significant-Gravitas/Auto, 2023\nAmmanabrolu P, Choi Y, Ren X. SwiftSage: a generative agent with 101. Chen L, Wang L, Dong H, Du Y, Yan J, Yang F, Li S, Zhao P, Qin S,\nfast and slow thinking for complex interactive tasks. In: Proceedings of Rajmohan S, Lin Q, Zhang D. Introspective tips: large language model\nthe 37th Conference on Neural Information Processing Systems. 2023, for in-context decision making. 2023, arXiv preprint arXiv:\n36 2305.11598\n83. Evans J S B T, Stanovich K E. Dual-process theories of higher 102. Aher G V, Arriaga R I, Kalai A T. Using large language models to\ncognition: advancing the debate. Perspectives on Psychological simulate multiple humans and replicate human subject studies. In:\nScience, 2013, 8(3): 223\u2013241 Proceedings of the 40th International Conference on Machine\n84. Liu R, Yang R, Jia C, Zhang G, Zhou D, Dai A M, Yang D, Vosoughi Learning. 2023, 337\u2212371\nS. Training socially aligned language models on simulated social 103. Akata E, Schulz L, Coda-Forno J, Oh S J, Bethge M, Schulz E.\ninteractions. 2023, arXiv preprint arXiv: 2305.16960 Playing repeated games with large language models. 2023, arXiv\n85. Weng X, Gu Y, Zheng B, Chen S, Stevens S, Wang B, Sun H, Su Y. preprint arXiv: 2305.16867\nMind2Web: towards a generalist agent for the Web. In: Proceedings of 104. Ma Z, Mei Y, Su Z. Understanding the benefits and challenges of\nthe 37th Conference on Neural Information Processing Systems. 2023, using large language model-based conversational agents for mental\n36 well-being support. In: Proceedings of AMIA Symposium. 2023,\n86. Sun R, Arik S O, Nakhost H, Dai H, Sinha R, Yin P, Pfister T. SQL- 1105\u22121114\nPaLm: improved large language model adaptation for text-to-SQL. 105. Ziems C, Held W, Shaikh O, Chen J, Zhang Z, Yang D. Can large\n2023, arXiv preprint arXiv: 2306.00739 language models transform computational social science? 2024, arXiv\n87. Yao W, Heinecke S, Niebles J C, Liu Z, Feng Y, Xue L, Murthy R, preprint arXiv: 2305.03514\nChen Z, Zhang J, Arpit D, Xu R, Mui P, Wang H, Xiong C, Savarese 106. Horton J J. Large language models as simulated economic agents: what\nS. Retroformer: retrospective large language agents with policy can we learn from homo silicus? 2023, arXiv preprint arXiv:\ngradient optimization, 2023, arXiv preprint arXiv: 2308.02151 2301.07543\n88. Shu Y, Zhang H, Gu H, Zhang P, Lu T, Li D, Gu N. RAH! RecSys- 107. Li S, Yang J, Zhao K. Are you in a masquerade? Exploring the\nassistant-human: a human-centered recommendation framework with behavior and impact of large language model driven social bots in\nLLM agents. 2023, arXiv preprint arXiv: 2308.09904 online social networks. 2023, arXiv preprint arXiv: 2307.10337\n89. Mandi Z, Jain S, Song S. RoCo: dialectic multi-robot collaboration 108. Li C, Su X, Han H, Xue C, Zheng C, Fan C. Quantifying the impact of\nwith large language models. 2023, arXiv preprint arXiv: 2307.04738 large language models on collective opinion dynamics. 2023, arXiv\n90. Zhang C, Liu L, Wang J, Wang C, Sun X, Wang H, Cai M. PREFER: preprint arXiv: 2308.03313\nprompt ensemble learning via feedback-reflect-refine. 2023, arXiv 109. Kova\u010d G, Portelas R, Dominey P F, Oudeyer P Y. The SocialAI",
          "similarity": 0.6108171939849854
        },
        {
          "content": "Lei WANG et al. A survey on large language model based autonomous agents 13\nTable 1 For the profile module, we use \u2460, \u2461, and \u2462 to represent the handcrafting method, LLM-generation method, and dataset alignment method,\nrespectively. For the memory module, we focus on the implementation strategies for memory operation and memory structure. For memory operation, we use\n\u2460 and \u2461 to indicate that the model only has read/write operations and has read/write/reflection operations, respectively. For memory structure, we use \u2460 and\n\u2461 to represent unified and hybrid memories, respectively. For the planning module, we use \u2460 and \u2461 to represent planning w/o feedback and w/ feedback,\nrespectively. For the action module, we use \u2460 and \u2461 to represent that the model does not use tools and use tools, respectively. For the agent capability\nacquisition (CA) strategy, we use \u2460 and \u2461 to represent the methods with and without fine-tuning, respectively. \u201c\u2212\u201d indicates that the corresponding content is\nnot explicitly discussed in the paper\nMemory\nModel Profile Planning Action CA Time\nOperation Structure\nWebGPT [66] \u2212 \u2212 \u2212 \u2212 \u2461 \u2460 12/2021\nSayCan [78] \u2212 \u2212 \u2212 \u2460 \u2460 \u2461 04/2022\nMRKL [72] \u2212 \u2212 \u2212 \u2460 \u2461 \u2212 05/2022\nInner Monologue [61] \u2212 \u2212 \u2212 \u2461 \u2460 \u2461 07/2022\nSocial Simulacra [98] \u2461 \u2212 \u2212 \u2212 \u2460 \u2212 08/2022\nReAct [59] \u2212 \u2212 \u2212 \u2461 \u2461 \u2460 10/2022\nMALLM [43] \u2212 \u2460 \u2461 \u2212 \u2460 \u2212 01/2023\nDEPS [33] \u2212 \u2212 \u2212 \u2461 \u2460 \u2461 02/2023\nToolformer [15] \u2212 \u2212 \u2212 \u2460 \u2461 \u2460 02/2023\nReflexion [12] \u2212 \u2461 \u2461 \u2461 \u2460 \u2461 03/2023\nCAMEL [99] \u2460 \u2461 \u2212 \u2212 \u2461 \u2460 \u2212 03/2023\nAPI-Bank [69] \u2212 \u2212 \u2212 \u2461 \u2461 \u2461 04/2023\nViperGPT [74] \u2212 \u2212 \u2212 \u2212 \u2461 \u2212 03/2023\nHuggingGPT [13] \u2212 \u2212 \u2460 \u2460 \u2461 \u2212 03/2023\nGenerative Agents [20] \u2460 \u2461 \u2461 \u2461 \u2460 \u2212 04/2023\nLLM+P [57] \u2212 \u2212 \u2212 \u2460 \u2460 \u2212 04/2023\nChemCrow [75] \u2212 \u2212 \u2212 \u2461 \u2461 \u2212 04/2023\nOpenAGI [73] \u2212 \u2212 \u2212 \u2461 \u2461 \u2460 04/2023\nAutoGPT [100] \u2212 \u2460 \u2461 \u2461 \u2461 \u2461 04/2023\nSCM [35] \u2212 \u2461 \u2461 \u2212 \u2460 \u2212 04/2023\nSocially Alignment [84] \u2212 \u2460 \u2461 \u2212 \u2460 \u2460 05/2023\nGITM [16] \u2212 \u2461 \u2461 \u2461 \u2460 \u2461 05/2023\nVoyager [38] \u2212 \u2461 \u2461 \u2461 \u2460 \u2461 05/2023\nIntrospective Tips [101] \u2212 \u2212 \u2212 \u2461 \u2460 \u2461 05/2023\nRET-LLM [42] \u2212 \u2460 \u2461 \u2212 \u2460 \u2460 05/2023\nChatDB [40] \u2212 \u2460 \u2461 \u2461 \u2461 \u2212 06/2023\nS3 [77] \u2462 \u2461 \u2461 \u2212 \u2460 \u2212 07/2023\nChatDev [18] \u2460 \u2461 \u2461 \u2461 \u2460 \u2461 07/2023\nToolLLM [14] \u2212 \u2212 \u2212 \u2461 \u2461 \u2460 07/2023\nMemoryBank [39] \u2212 \u2461 \u2461 \u2212 \u2460 \u2212 07/2023\nMetaGPT [23] \u2460 \u2461 \u2461 \u2461 \u2461 \u2212 08/2023\nFig. 5 The applications (left) and evaluation strategies (right) of LLM-based agents\n[102\u2013105]. For example, in [102], the authors assign LLMs involving human participants. Additionally, it was observed\nwith different profiles, and let them complete psychology that larger models tend to deliver more accurate simulation\nexperiments. From the results, the authors find that LLMs are results compared to their smaller counterparts. An interesting\ncapable of generating results that align with those from studies discovery is that, in many experiments, models like ChatGPT",
          "similarity": 0.6072437763214111
        },
        {
          "content": "Lei WANG et al. A survey on large language model based autonomous agents 25\n156. Zhang C, Yang K, Hu S, Wang Z, Li G, Sun Y, Zhang C, Zhang Z, Liu In: Proceedings of 2023 Conference on Empirical Methods in Natural\nA, Zhu S C, Chang X, Zhang J, Yin F, Liang Y, Yang Y. ProAgent: Language Processing: System Demonstrations. 2023, 237\u2212245\nbuilding proactive cooperative agents with large language models. 173. Chalamalasetti K, G\u00f6tze J, Hakimov S, Madureira B, Sadler P,\n2024, arXiv preprint arXiv: 2308.11339 Schlangen D. clembench: Using game play to evaluate chat-optimized\n157. Xiang J, Tao T, Gu Y, Shu T, Wang Z, Yang Z, Hu Z. Language language models as conversational agents. In: Proceedings of 2023\nmodels meet world models: embodied experiences enhance language Conference on Empirical Methods in Natural Language Processing.\nmodels. In: Proceedings of the 37th Conference on Neural Information 2023, 11174\u201311219\nProcessing Systems. 2023, 36 174. Banerjee D, Singh P, Avadhanam A, Srivastava S. Benchmarking\n158. Lee M, Srivastava M, Hardy A, Thickstun J, Durmus E, Paranjape A, LLM powered chatbots: methods and metrics. 2023, arXiv preprint\nGerard-Ursin I, Li X L, Ladhak F, Rong F, Wang R E, Kwon M, Park arXiv: 2308.04624\nJ S, Cao H, Lee T, Bommasani R, Bernstein M, Liang P. Evaluating 175. Lin J, Tomlin N, Andreas J, Eisner J. Decision-oriented dialogue for\nhuman-language model interaction. 2024, arXiv preprint arXiv: human-AI collaboration. 2023, arXiv preprint arXiv: 2305.20076\n2212.09746 176. Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B,\n159. Krishna R, Lee D, Fei-Fei L, Bernstein M S. Socially situated artificial Zhang J, Dong Z, Du Y, Yang C, Chen Y, Chen Z, Jiang J, Ren R, Li\nintelligence enables learning from human interaction. Proceedings of Y, Tang X, Liu Z, Liu P, Nie J Y, Wen J R. A survey of large language\nthe National Academy of Sciences of the United States of America, models. 2023, arXiv preprint arXiv: 2303.18223\n2022, 119(39): e2115730119\n177. Yang J, Jin H, Tang R, Han X, Feng Q, Jiang H, Zhong S, Yin B, Hu\n160. Huang J T, Lam M H, Li E J, Ren S, Wang W, Jiao W, Tu Z, Lyu M\nX. Harnessing the power of LLMs in practice: a survey on chatGPT\nR. Emotionally numb or empathetic? Evaluating how LLMs feel using\nand beyond. ACM Transactions on Knowledge Discovery from Data,\nemotionbench. 2024, arXiv preprint arXiv: 2308.03656\n2024, doi: 10.1145/3649506\n161. Chan C M, Chen W, Su Y, Yu J, Xue W, Zhang S, Fu J, Liu Z.\n178. Wang Y, Zhong W, Li L, Mi F, Zeng X, Huang W, Shang L, Jiang X,\nChatEval: towards better LLM-based evaluators through multi-agent\nLiu Q. Aligning large language models with human: a survey. 2023,\ndebate. 2023, arXiv preprint arXiv: 2308.07201\narXiv preprint arXiv: 2307.12966\n162. Chen A, Phang J, Parrish A, Padmakumar V, Zhao C, Bowman S R,\n179. Huang J, Chang K C C. Towards reasoning in large language models:\nCho K. Two failures of self-consistency in the multi-step reasoning of\na survey. In: Proceedings of Findings of the Association for\nLLMs. 2024, arXiv preprint arXiv: 2305.14279\nComputational Linguistics: ACL 2023. 2023, 1049\u20131065\n163. Zhang D, Xu H, Zhao Z, Chen L, Cao R, Yu K. Mobile-env: an\n180. Mialon G, Dess\u00ec R, Lomeli M, Nalmpantis C, Pasunuru R, Raileanu R,\nevaluation platform and benchmark for LLM-GUI interaction. 2024,\nRozi\u00e8re B, Schick T, Dwivedi-Yu J, Celikyilmaz A, Grave E, LeCun\narXiv preprint arXiv: 2305.08144\nY, Scialom T. Augmented language models: a survey. 2023, arXiv\n164. Liang Y, Zhu L, Yang Y. Tachikuma: understading complex\npreprint arXiv: 2302.07842\ninteractions with multi-character and novel objects by large language\n181. Chang Y, Wang X, Wang J, Wu Y, Yang L, Zhu K, Chen H, Yi X,\nmodels. 2023, arXiv preprint arXiv: 2307.12573\nWang C, Wang Y, Ye W, Zhang Y, Chang Y, Yu P S. A survey on\n165. Choi M, Pei J, Kumar S, Shu C, Jurgens D. Do LLMs understand\nevaluation of large language models. ACM Transactions on Intelligent\nsocial knowledge? Evaluating the sociability of large language models\nSystems and Technology, 2023, doi: 10.1145/3641289\nwith socKET benchmark. In: Proceedings of 2023 Conference on\n182. Chang T A, Bergen B K. Language model behavior: a comprehensive\nEmpirical Methods in Natural Language Processing. 2023,\nsurvey. Computational Linguistics, 2024, doi: 10.1162/coli_a_00492\n11370\u201311403\n183. Li C, Wang J, Zhu K, Zhang Y, Hou W, Lian J, Xie X.\n166. Liu Z, Yao W, Zhang J, Xue L, Heinecke S, Murthy R, Feng Y, Chen\nEmotionprompt: Leveraging psychology for large language models\nZ, Niebles J C, Arpit D, Xu R, Mui P, Wang H, Xiong C, Savarese S.\nenhancement via emotional stimulus. 2023, arXiv preprint arXiv:\nBOLAA: benchmarking and orchestrating LLM-augmented\n2307.11760\nautonomous agents. 2023, arXiv preprint arXiv: 2308.05960\n184. Zhuo T Y, Li Z, Huang Y, Shiri F, Wang W, Haffari G, Li Y F. On\n167. Liu X, Yu H, Zhang H, Xu Y, Lei X, Lai H, Gu Y, Ding H, Men K,\nYang K, Zhang S, Deng X, Zeng A, Du Z, Zhang C, Shen S, Zhang T, robustness of prompt-based semantic parsing with large pre-trained\nSu Y, Sun H, Huang M, Dong Y, Tang J. AgentBench: evaluating language model: an empirical study on codex. In: Proceedings of the\nLLMs as agents. 2023, arXiv preprint arXiv: 2308.03688 17th Conference of the European Chapter of the Association for\n168. Kang S, Yoon J, Yoo S. Large language models are few-shot testers: Computational Linguistics. 2023, 1090\u20131102\nexploring LLM-based general bug reproduction. In: Proceedings of the 185. Gekhman Z, Oved N, Keller O, Szpektor I, Reichart R. On the\n45th IEEE/ACM International Conference on Software Engineering. robustness of dialogue history representation in conversational\n2023, 2312\u22122323 question answering: a comprehensive study and a new prompt-based\n169. Jalil S, Rafi S, LaToza T D, Moran K, Lam W. ChatGPT and software method. Transactions of the Association for Computational\ntesting education: Promises & perils. In: Proceedings of 2023 IEEE Linguistics, 2023, 11(11): 351\u2013366\nInternational Conference on Software Testing, Verification and 186. Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y, Ishii E, Bang Y J, Madotto\nValidation Workshops. 2023, 4130\u22124137 A, Fung P. Survey of hallucination in natural language generation.\n170. Feldt R, Kang S, Yoon J, Yoo S. Towards autonomous testing agents ACM Computing Surveys, 2023, 55(12): 248\nvia conversational large language models. In: Proceedings of the 38th\nIEEE/ACM International Conference on Automated Software Lei Wang is a PhD candidate at Renmin\nEngineering. 2023, 1688\u22121693\nUniversity of China, China. His research focuses\n171. Zhou S, Xu F F, Zhu H, Zhou X, Lo R, Sridhar A, Cheng X, Ou T,\non recommender systems and agent-based large\nBisk Y, Fried D, Alon U, Neubig G. WebArena: a realistic Web\nlanguage models.\nenvironment for building autonomous agents. 2023, arXiv preprint\narXiv: 2307.13854\n172. Xu B, Liu X, Shen H, Han Z, Li Y, Yue M, Peng Z, Liu Y, Yao Z, Xu\nD. Gentopia.AI: a collaborative platform for tool-augmented LLMs.",
          "similarity": 0.6013684272766113
        }
      ],
      "paragraph_count": 26
    },
    {
      "title": "Generative artificial intelligence in smart manufacturing",
      "authors": "Kusiak, Andrew",
      "year": "2024",
      "doi": "10.1007/s10845-024-02480-6",
      "filepath": "papers/Generative artificial intelligence in smart manufacturing_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 81
    },
    {
      "title": "Tackling vision language tasks through learning inner monologues",
      "authors": "Yang, Diji and Chen, Kezhen and Rao, Jinmeng and Guo, Xiaoyuan and Zhang, Yawen and Yang, Jie and Zhang, Yi",
      "year": "2024",
      "doi": "10.1609/aaai.v38i17.29905",
      "filepath": "papers/Tackling vision language tasks through learning inner monologues_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 9
    },
    {
      "title": "LLM-powered synthetic environments for self-driving scenarios",
      "authors": "Adekanye, Oluwanifemi Adebayo Moses",
      "year": "2024",
      "doi": "10.1609/aaai.v38i21.30540",
      "filepath": "papers/LLM-powered synthetic environments for self-driving scenarios_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 47
    },
    {
      "title": "Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL",
      "authors": "Zhong, Fangwei and Wu, Kui and Ci, Hai and Wang, Churan and Chen, Hao",
      "year": "2024",
      "doi": "10.1007/978-3-031-73464-9_9",
      "filepath": "papers/Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 10
    },
    {
      "title": "A survey of table reasoning with large language models",
      "authors": "Zhang, Xuanliang and Wang, Dingzirui and Dou, Longxu and Zhu, Qingfu and Che, Wanxiang",
      "year": "2025",
      "doi": "10.1007/s11704-024-40330-z",
      "filepath": "papers/A survey of table reasoning with large language models_2025.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 7
    },
    {
      "title": "Integrating Ontologies and Cognitive Conversational Agents in On2Conv",
      "authors": "Esfahani, Zeinab Namakizadeh and Engelmann, D\\'{e",
      "year": "2023",
      "doi": "10.1007/978-3-031-43264-4_5",
      "filepath": "papers/Integrating Ontologies and Cognitive Conversational Agents in On2Conv_2023.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 10
    },
    {
      "title": "Adaptive Human Trajectory Prediction via Latent Corridors",
      "authors": "Thakkar, Neerja and Mangalam, Karttikeya and Bajcsy, Andrea and Malik, Jitendra",
      "year": "2024",
      "doi": "10.1007/978-3-031-72920-1_17",
      "filepath": "papers/Adaptive Human Trajectory Prediction via Latent Corridors_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 6
    },
    {
      "title": "Evolution of Neural Networks",
      "authors": "Miikkulainen, Risto",
      "year": "2024",
      "doi": "10.1145/3638530.3648407",
      "filepath": "papers/Evolution of Neural Networks_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 18
    },
    {
      "title": "Human-guided moral decision making in text-based games",
      "authors": "Shi, Zijing and Fang, Meng and Chen, Ling and Du, Yali and Wang, Jun",
      "year": "2024",
      "doi": "10.1609/aaai.v38i19.30155",
      "filepath": "papers/Human-guided moral decision making in text-based games_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 9
    },
    {
      "title": "Weak-to-Strong Compositional Learning from Generative Models for Language-Based Object Detection",
      "authors": "Park, Kwanyong and Saito, Kuniaki and Kim, Donghyun",
      "year": "2024",
      "doi": "10.1007/978-3-031-73337-6_1",
      "filepath": "papers/Weak-to-Strong Compositional Learning from Generative Models for Language-Based Object Detection_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 6
    },
    {
      "title": "Removing Distributional Discrepancies in Captions Improves Image-Text Alignment",
      "authors": "Li, Yuheng and Liu, Haotian and Cai, Mu and Li, Yijun and Shechtman, Eli and Lin, Zhe and Lee, Yong Jae and Singh, Krishna Kumar",
      "year": "2024",
      "doi": "10.1007/978-3-031-72664-4_23",
      "filepath": "papers/Removing Distributional Discrepancies in Captions Improves Image-Text Alignment_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 18
    },
    {
      "title": "A conversational agent for creating automations exploiting large language models",
      "authors": "Gallo, Simone and Patern\\`{o",
      "year": "2024",
      "doi": "10.1007/s00779-024-01825-5",
      "filepath": "papers/A conversational agent for creating automations exploiting large language models_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 16
    },
    {
      "title": "Enhancing Large Language Models Through External Domain Knowledge",
      "authors": "Welz, Laslo and Lanquillon, Carsten",
      "year": "2024",
      "doi": "10.1007/978-3-031-60615-1_9",
      "filepath": "papers/Enhancing Large Language Models Through External Domain Knowledge_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 53
    },
    {
      "title": "Perception-based constraint solving for sudoku images",
      "authors": "Mulamba, Maxime and Mandi, Jayanta and Mahmuto\\u{g",
      "year": "2024",
      "doi": "10.1007/s10601-024-09372-9",
      "filepath": "papers/Perception-based constraint solving for sudoku images_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 40
    },
    {
      "title": "Towards automatic boundary detection for human-AI collaborative hybrid essay in education",
      "authors": "Zeng, Zijie and Sha, Lele and Li, Yuheng and Yang, Kaixun and Ga\\v{s",
      "year": "2024",
      "doi": "10.1609/aaai.v38i20.30258",
      "filepath": "papers/Towards automatic boundary detection for human-AI collaborative hybrid essay in education_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 9
    },
    {
      "title": "Enhancing smart home interaction through multimodal command disambiguation",
      "authors": "Cal\\`{o",
      "year": "2024",
      "doi": "10.1007/s00779-024-01827-3",
      "filepath": "papers/Enhancing smart home interaction through multimodal command disambiguation_2024.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 16
    },
    {
      "title": "MALACHITE - Enabling Users to Teach GUI-Aware Natural Language Interfaces",
      "authors": "Ruoff, Marcel and Myers, Brad A. and Maedche, Alexander",
      "year": "2025",
      "doi": "10.1145/3716141",
      "filepath": "papers/MALACHITE - Enabling Users to Teach GUI-Aware Natural Language Interfaces_2025.pdf",
      "relevance_score": 0.0,
      "relevant_paragraphs": [],
      "paragraph_count": 29
    }
  ]
}